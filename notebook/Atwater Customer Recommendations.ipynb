{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atwater Customer Recommendations\n",
    "\n",
    "#### A demo using DataStax Enterprise Analytics, Apache Cassandra, Apache Spark, Python and Jupyter Notebooks to utilize the power of big customer data to recommend items to our customers with a high degree of accruacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things To Setup\n",
    "* Install DSE https://docs.datastax.com/en/install/doc/install60/installTOC.html\n",
    "* Start DSE Analytics Cluster\n",
    "* Using Python 2.7\n",
    "* Using DSE Analytics 6\n",
    "* Using latest verion of Jupyter \n",
    "* Find full path to <>/lib/pyspark.zip\n",
    "* Find full path to <>/lib/py4j-0.10.4-src.zip\n",
    "* Start Jupyter with DSE to get all environemnt variables: dse exec jupyter notebook\n",
    "* Make sure that the all the CSV files are in the same locations as this notebook\n",
    "* Make sure that all \\*.cql files are in the same locations as this notebook\n",
    "* !pip install cassandra-driver\n",
    "* !pip install pattern \n",
    "* !pip install panadas\n",
    "* Counter-intuitive don't install pyspark!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add some environment variables to find dse verision of pyspark. Edit these varibles with your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkzip = \"/usr/share/dse/spark/python/lib/pyspark.zip\"\n",
    "py4jzip = \"/usr/share/dse/spark/python/lib/py4j-0.10.4-src.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to be able to find pyspark libaries\n",
    "import sys\n",
    "sys.path.append(pysparkzip)\n",
    "sys.path.append(py4jzip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import python packages -- all are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import cassandra\n",
    "import pyspark\n",
    "import re\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pattern.en import sentiment, positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to have nicer formatting of Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for pretty formatting for Spark DataFrames\n",
    "def showDF(df, limitRows =  5, truncate = True):\n",
    "    if(truncate):\n",
    "        pandas.set_option('display.max_colwidth', 50)\n",
    "    else:\n",
    "        pandas.set_option('display.max_colwidth', -1)\n",
    "    pandas.set_option('display.max_rows', limitRows)\n",
    "    display(df.limit(limitRows).toPandas())\n",
    "    pandas.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tables, Pulling Tweets, and Loading Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to DSE Analytics Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['127.0.0.1']) #If you have a locally installed DSE cluster\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Demo Keyspace --Replication Factor is 1 since only have a one node demo cluster. Replication Factor is recommended at 3 or Write Consistency + Read Consistency > Replication Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS demo \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace('demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the customer transaction table in DSE (this is for completed transactions).  This table will be updated with about 1000 transactions a minute (Atwater has around 200,000 transactions a day on their website)\n",
    "#### Our primary key will be on state (limiting our analysis to just the US), and our clustering columns will be around gender, age and the transaction id. Consider your data model when choosing your primary key. This will give us a good distriubtion of the data and a unique row for each transaction. We will also be able to create models around age, gender, and state to give the best possible recommendations. \n",
    "\n",
    "##### For this demo this table will store 1 million records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS customer_transactions (id int, \\\n",
    "                                                            customer_name text, \\\n",
    "                                                            gender text, age int, \\\n",
    "                                                            state text, home_store int, \\\n",
    "                                                            items list<text>, year int, \\\n",
    "                                                            month int, rewards_member text, \\\n",
    "                                                            PRIMARY KEY ((state), gender, age, id))\"\n",
    "session.execute(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the live customer table in DSE - this represents live customers that are currently logged in on the site and what they have in their shopping cart. \n",
    "#### We will use that information to get a prediction of what we should recommend for them.  The data model is the same as above. \n",
    "##### For the ease of the demo the items per cart will just be 1 item. For demo this table will hold 10K records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS customer_live (id int, \\\n",
    "                                                            customer_name text, \\\n",
    "                                                            gender text, age int, \\\n",
    "                                                            state text, home_store int, \\\n",
    "                                                            items list<text>, year int, \\\n",
    "                                                            month int, rewards_member text, \\\n",
    "                                                            PRIMARY KEY ((state), gender, age, id))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Customer Recommendation Table in DSE\n",
    "#### This table will be used with the inventory table to show the correct, in-stock items by the website. --In reality this information probably would not be written back to a Cassandra table as it doesn't need to be stored long-term. For the shake of the demo showing fast reads and fast writes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS customer_recommend (id int, \\\n",
    "                                                            customer_name text, \\\n",
    "                                                            gender text, age int, \\\n",
    "                                                            state text, home_store int, \\\n",
    "                                                            items list<text>, year int, \\\n",
    "                                                            month int, rewards_member text,\\\n",
    "                                                            prediction list<text>,\\\n",
    "                                                            PRIMARY KEY ((id, state), gender, age))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Inventory Table in DSE \n",
    "#### Our primary key is going to be around the item type (pants, shirts, blender), the location of the items, the sku, and if it the items is currently avaliable. While customers may want to look at items that are on back-order, we do not want to recommend them. This will only cause frustration. This table would have around 6 million entries at one time, with inserts/deletions daily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS inventory (sku int, \\\n",
    "                                               item_name text, item_type text, \\\n",
    "                                               stock_loc text, num_items int, \\\n",
    "                                               backorder text, \\\n",
    "                                               PRIMARY KEY (item_type, stock_loc, backorder, sku))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CSV files into DSE for Customer Transactions, Customer Live/Shopping Cart and Inventory Tables\n",
    "##### Note could also use bulk loader or a loop with insert statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 customer.csv\n",
    "!cat loadCustomer.cql\n",
    "!time cqlsh -f loadCustomer.cql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 customerTest.csv\n",
    "!cat loadCustomerTest.cql\n",
    "!time cqlsh -f loadCustomerTest.cql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 inventory.csv\n",
    "!cat loadInventory.cql\n",
    "!cqlsh -f loadInventory.cql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a select * on customer transaction table and verify that the values have been inserted into the DSE table. Because we have used as our primary key \"State\" we can use this in our WHERE clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = 'SELECT * FROM customer_transactions WHERE state=\\'CA\\' limit 10'\n",
    "rows = session.execute(query)\n",
    "for user_row in rows:\n",
    "    print (user_row.id, user_row.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally time for Some Analytics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a spark session that is connected to Cassandra. From there load each table into a Spark Dataframe and take a count of the number of rows in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "\n",
    "tableDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_transactions\", keyspace=\"demo\").load()\n",
    "\n",
    "testDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_live\", keyspace=\"demo\").load()\n",
    "\n",
    "\n",
    "print \"Table Train Count: \"\n",
    "print tableDF.count()\n",
    "showDF(tableDF)\n",
    "\n",
    "print \"Table Test Count: \"\n",
    "print testDF.count()\n",
    "showDF(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Spark we can also quickly create a new dataframes with just Customers who are of the ages 18-23. \n",
    "#### We can use this data frame to create another model and score our live data against.  \n",
    "#### Models++ = Strong Accurancy --> $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableMilDF = tableDF.where(tableDF.age < 24)\n",
    "print \"Table Milennial Train Count: \"\n",
    "print tableMilDF.count()\n",
    "showDF(tableMilDF)\n",
    "testMilDF = testDF.where(testDF.age < 24)\n",
    "print \"Table Milennial Test Count:\"\n",
    "print testMilDF.count()\n",
    "showDF(testMilDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table24DF = tableDF.where(tableDF.age > 24)\n",
    "print \"Table Everyone Else Train Count: \"\n",
    "print table24DF.count()\n",
    "showDF(table24DF)\n",
    "print \"Table Everyone Else Test Count:\"\n",
    "test24DF = testDF.where(testDF.age > 24)\n",
    "print test24DF.count()\n",
    "showDF(test24DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPGROWTH for Customer Shopping Cart Recommendations\n",
    "#### Use Apache Spark MLlib with FPGrowth to find Recommendation -- Do model training on customer transaction dataset then use the live customer shopping cart as the test dataset\n",
    "#### https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html\n",
    "#### https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.fpm.FPGrowth\n",
    "\n",
    "#### Let's look at Non-Milennials First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.1, minConfidence=0.2)\n",
    "model = fpGrowth.fit(table24DF)\n",
    "recommendDF=model.transform(test24DF)\n",
    "recommendDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a model just for our young Milennial Customers\n",
    "#### Did we see anything different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpGrowth1 = FPGrowth(itemsCol=\"items\", minSupport=0.1, minConfidence=0.1)\n",
    "model1 = fpGrowth1.fit(tableMilDF)\n",
    "recommendDF1=model1.transform(testMilDF)\n",
    "recommendDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "dfPrediction1=recommendDF1.where(size(col(\"prediction\")) > 0)\n",
    "dfPrediction=recommendDF.where(size(col(\"prediction\")) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Recommendations Dataframe include Predictions back into a Cassandra Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPrediction1.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_recommend\", keyspace=\"demo\").save(mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPrediction.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"customer_recommend\", keyspace=\"demo\").save(mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recommended Items from Inventory Table\n",
    "#### For each customer in the recommendations table (sorry this is a select * on all, but must be done!) For each item type that was recommended (bed, sweather, pants) query the inventory table for that item type, the wearhouse location that is in the same state at the user (one per state), and if the item is on backorder. Whatever specific item name matchs this query is what will be recommended to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = 'SELECT * FROM customer_recommend limit 10'\n",
    "rows = session.execute(query)\n",
    "for user_row in rows:\n",
    "        for item in user_row.prediction:\n",
    "            query = \"SELECT * FROM inventory WHERE item_type=\\'%s\\' AND stock_loc=\\'%s\\' AND\\\n",
    "            backorder=\\'N\\'\" % (item, user_row.state)\n",
    "            items = session.execute(query)\n",
    "            for item_row in items:\n",
    "                print \"Customer: \" + user_row.customer_name + \" **Shopping Cart: \" + str(user_row.items) + \"** --> Recommendations: Type: \" + item + \" Name: \" + item_row.item_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.execute(\"drop table customer_live\")\n",
    "#session.execute(\"drop table customer_recommend\")\n",
    "#session.execute(\"drop table inventory\")\n",
    "#session.execute(\"drop table customer_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
